{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.svm import SVC\n",
    "#from lightgbm import LGBMClassifier\n",
    "#from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diet = pd.read_csv(\"data/diet.csv\")\n",
    "df_recipes = pd.read_csv(\"data/recipes.csv\")\n",
    "df_requests = pd.read_csv(\"data/requests.csv\")\n",
    "df_reviews = pd.read_csv(\"data/reviews.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1. Extract from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' df_recipes_quantity = df_recipes.copy()\\ncleaned_quantity_list = []\\n\\nfor i in df_recipes_quantity.index:\\n    cleaned_ingredient_string = df_recipes_quantity[\\'RecipeIngredientQuantities\\'][i].replace(\\'c(\\', \\'\\').replace(\\'\\\\\"\\', \\'\\').replace(\\'\"\\', \\'\\').replace(\\')\\',\\'\\')\\n    cleaned_quantity_list.append({\\'CleanedQuantities\\': cleaned_ingredient_string}) \\n    \\nfrom fractions import Fraction\\ndef parse_quantity(quantity_str):\\n    if quantity_str is None:\\n        return None\\n    try:\\n        # Handle mixed numbers (e.g., \\'1 1/2\\')\\n        if \\' \\' in quantity_str:\\n            whole, frac = quantity_str.split(\\' \\')\\n            return float(whole) + float(Fraction(frac))\\n        # Handle fractions in ranges (e.g., \\'1/2 - 3/4\\' or \\'1/8 - 1/4\\')\\n        elif \\'-\\' in quantity_str and \\'/\\' in quantity_str:\\n            range_values = re.findall(r\\'\\x08\\\\d+\\\\s*\\\\d*/\\\\d+\\x08\\', quantity_str)\\n            range_values = [float(Fraction(val.strip())) for val in range_values]\\n            return sum(range_values) / len(range_values)\\n        elif \\'/\\' in quantity_str:\\n            return float(Fraction(quantity_str))\\n        elif \\'-\\' in quantity_str:\\n            # Handle the case of a range\\n            range_values = list(map(float, quantity_str.split(\\'-\\')))\\n            return sum(range_values) / len(range_values)\\n        else:\\n            return float(quantity_str)\\n    except ValueError:\\n        # Handle cases where the quantity cannot be converted to a float\\n        #print(f\"Unable to convert \\'{quantity_str}\\' to a float\")\\n        return None'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Extract ingredientes from source format\n",
    "df_recipes_ingredients = df_recipes.copy()\n",
    "cleaned_ingredients_list = []\n",
    "\n",
    "for i in df_recipes_ingredients.index:\n",
    "    cleaned_ingredient_string = df_recipes_ingredients['RecipeIngredientParts'][i].replace('c(', '').replace('\\\\\"', '').replace('\"', '').replace(')','')\n",
    "    cleaned_ingredients_list.append({'CleanedIngredients': cleaned_ingredient_string})\n",
    "\n",
    "cleaned_df_ingredient = pd.DataFrame(cleaned_ingredients_list)\n",
    "\n",
    "## Cleaning of the quantitites\n",
    "\"\"\" df_recipes_quantity = df_recipes.copy()\n",
    "cleaned_quantity_list = []\n",
    "\n",
    "for i in df_recipes_quantity.index:\n",
    "    cleaned_ingredient_string = df_recipes_quantity['RecipeIngredientQuantities'][i].replace('c(', '').replace('\\\\\"', '').replace('\"', '').replace(')','')\n",
    "    cleaned_quantity_list.append({'CleanedQuantities': cleaned_ingredient_string}) \n",
    "    \n",
    "from fractions import Fraction\n",
    "def parse_quantity(quantity_str):\n",
    "    if quantity_str is None:\n",
    "        return None\n",
    "    try:\n",
    "        # Handle mixed numbers (e.g., '1 1/2')\n",
    "        if ' ' in quantity_str:\n",
    "            whole, frac = quantity_str.split(' ')\n",
    "            return float(whole) + float(Fraction(frac))\n",
    "        # Handle fractions in ranges (e.g., '1/2 - 3/4' or '1/8 - 1/4')\n",
    "        elif '-' in quantity_str and '/' in quantity_str:\n",
    "            range_values = re.findall(r'\\b\\d+\\s*\\d*/\\d+\\b', quantity_str)\n",
    "            range_values = [float(Fraction(val.strip())) for val in range_values]\n",
    "            return sum(range_values) / len(range_values)\n",
    "        elif '/' in quantity_str:\n",
    "            return float(Fraction(quantity_str))\n",
    "        elif '-' in quantity_str:\n",
    "            # Handle the case of a range\n",
    "            range_values = list(map(float, quantity_str.split('-')))\n",
    "            return sum(range_values) / len(range_values)\n",
    "        else:\n",
    "            return float(quantity_str)\n",
    "    except ValueError:\n",
    "        # Handle cases where the quantity cannot be converted to a float\n",
    "        #print(f\"Unable to convert '{quantity_str}' to a float\")\n",
    "        return None\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2. Create separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'CleanedIngredients' column into separate columns\n",
    "df_split_i = cleaned_df_ingredient['CleanedIngredients'].str.split(', ', expand=True)\n",
    "\"\"\"df_split_q = cleaned_df_quantity['CleanedQuantities'].str.split(', ', expand=True) \"\"\"\n",
    "# Rename the columns to include the original index\n",
    "df_split_i.columns = [f'Ingredient_{i}' for i in range(df_split_i.shape[1])]\n",
    "\"\"\"df_split_q.columns = [f'Quantity_{i}' for i in range(df_split_q.shape[1])] \"\"\"\n",
    "# Concatenate the original DataFrame with the new columns\n",
    "df_result_i = pd.concat([cleaned_df_ingredient, df_split_i], axis=1)\n",
    "\"\"\"df_result_q = pd.concat([cleaned_df_quantity, df_split_q], axis=1) \"\"\"\n",
    "\n",
    "# Apply the parse_quantity function to each 'Quantity' column\n",
    "\"\"\" for col in df_result_q.columns:\n",
    "    df_result_q[col] = df_result_q[col].apply(parse_quantity) \"\"\"\n",
    "\n",
    "# Stack the DataFrame and create a new DataFrame\n",
    "unique_ingredients_df = pd.DataFrame(df_result_i.stack().reset_index(drop=True), columns=['Ingredients'])\n",
    "#unique_ingredients_df['Diet']='vegan'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3. Manual diet mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Define keywords for different diet types\\ndiet_keywords = {\\n    'Vegetarian': ['milk', 'butter', 'egg','buttermilk biscuits', 'ricotta cheese', 'custard', 'clarified butter',\\n    'queso fresco', 'gouda cheese', 'brie cheese', 'halloumi cheese','cheese','chocolate'\\n    'caramel', 'creme brulee', 'eggnog', 'mango lassi', 'creamy dressings','eggs'\\n    'hollandaise sauce', 'anchovy paste', 'pâté',\\n    'scallops', 'clam chowder', 'crab cakes', 'crème fraîche', 'gravlax', \\n    'gelato', 'honey mustard', 'miso butter', 'spicy mayo',\\n    'burrata cheese', 'whipped honey butter',  'deviled eggs', \\n    'pumpkin cheesecake', 'goat cheese-stuffed figs', 'crispy prosciutto',\\n    'chocolate mousse', 'garlic butter mushrooms', 'pesto pasta with parmesan'\\n    'sour cream', 'cottage cheese', 'mayonnaise', 'whipped cream',\\n    'ghee', 'paneer', 'parmesan cheese', 'mozzarella cheese',\\n    'feta cheese', 'goat cheese', 'blue cheese', 'cream cheese',\\n    'buttermilk', 'marshmallows', 'white chocolate', 'casein',\\n    'whey', 'honeycomb', 'royal jelly', 'propolis',\\n    'egg noodles', 'ravioli with cheese filling', 'honey mustard dressing',\\n    'popcorn with butter', 'french onion dip', 'onion soup with cheese',\\n    'chocolate-covered pretzels', 'caramel popcorn', 'milk chocolate',\\n    'cheese fondue', 'risotto with parmesan', 'strawberries and cream',\\n    'egg salad', 'cheese soufflé', 'cheesecake', 'honey butter',\\n    'buttermilk pancakes', 'mango lassi with yogurt', 'cream-based sauces',\\n    'buttery croissants', 'garlic butter shrimp', 'lobster with butter',\\n    'baked brie with honey and nuts'],\\n\\n\\n    'Omnivore': ['beef', 'chicken', 'fish', 'beef', 'chicken', 'pork', 'lamb', 'veal', 'duck', 'turkey', 'rabbit',\\n    'bacon', 'sausage', 'ham', 'salami', 'pepperoni', 'prosciutto',\\n    'fish', 'shrimp', 'lobster', 'crab', 'clams', 'mussels', 'oysters',\\n    'anchovies', 'sardines', 'herring', 'tuna', 'caviar', 'escargot',\\n    'bone broth', 'lard', 'schmaltz', 'animal fat', 'ghee']\\n\\n    # Add more diet types and their associated keywords as needed\\n}\\n\\n# Function to determine diet type based on keywords\\ndef infer_diet(ingredients):\\n    omnivore_keywords = diet_keywords.get('Omnivore', [])\\n    vegetarian_keywords = diet_keywords.get('Vegetarian', [])\\n\\n    # Check for omnivore keywords first\\n    if any(keyword in ingredients.lower() for keyword in omnivore_keywords):\\n        return 'Omnivore'\\n    # Check for vegetarian keywords if not omnivore\\n    elif any(keyword in ingredients.lower() for keyword in vegetarian_keywords):\\n        return 'Vegetarian'\\n    else:\\n        return 'Vegan'  # Default to Vegan if no diet type is inferred\\n    \\n# Apply the function to create the 'Diet' column\\ncleaned_df_ingredient['meat_vegan_veggie'] = cleaned_df_ingredient['CleanedIngredients'].apply(infer_diet)\\ncleaned_df_ingredient['RecipeId'] = df_recipes['RecipeId']\\ncleaned_df_ingredient['Omnivore'] = 0\\ncleaned_df_ingredient['Vegeterian'] = 0\\ncleaned_df_ingredient['Vegan'] = 0\\ncleaned_df_ingredient.loc[cleaned_df_ingredient['meat_vegan_veggie'] == 'Omnivore', 'Omnivore'] = 1\\ncleaned_df_ingredient.loc[cleaned_df_ingredient['meat_vegan_veggie'] == 'Vegan', ['Omnivore','Vegeterian','Vegan']] = 1\\ncleaned_df_ingredient.loc[cleaned_df_ingredient['meat_vegan_veggie'] == 'Vegeterian', ['Vegeterian','Omnivore']] = 1 \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # Define keywords for different diet types\n",
    "diet_keywords = {\n",
    "    'Vegetarian': ['milk', 'butter', 'egg','buttermilk biscuits', 'ricotta cheese', 'custard', 'clarified butter',\n",
    "    'queso fresco', 'gouda cheese', 'brie cheese', 'halloumi cheese','cheese','chocolate'\n",
    "    'caramel', 'creme brulee', 'eggnog', 'mango lassi', 'creamy dressings','eggs'\n",
    "    'hollandaise sauce', 'anchovy paste', 'pâté',\n",
    "    'scallops', 'clam chowder', 'crab cakes', 'crème fraîche', 'gravlax', \n",
    "    'gelato', 'honey mustard', 'miso butter', 'spicy mayo',\n",
    "    'burrata cheese', 'whipped honey butter',  'deviled eggs', \n",
    "    'pumpkin cheesecake', 'goat cheese-stuffed figs', 'crispy prosciutto',\n",
    "    'chocolate mousse', 'garlic butter mushrooms', 'pesto pasta with parmesan'\n",
    "    'sour cream', 'cottage cheese', 'mayonnaise', 'whipped cream',\n",
    "    'ghee', 'paneer', 'parmesan cheese', 'mozzarella cheese',\n",
    "    'feta cheese', 'goat cheese', 'blue cheese', 'cream cheese',\n",
    "    'buttermilk', 'marshmallows', 'white chocolate', 'casein',\n",
    "    'whey', 'honeycomb', 'royal jelly', 'propolis',\n",
    "    'egg noodles', 'ravioli with cheese filling', 'honey mustard dressing',\n",
    "    'popcorn with butter', 'french onion dip', 'onion soup with cheese',\n",
    "    'chocolate-covered pretzels', 'caramel popcorn', 'milk chocolate',\n",
    "    'cheese fondue', 'risotto with parmesan', 'strawberries and cream',\n",
    "    'egg salad', 'cheese soufflé', 'cheesecake', 'honey butter',\n",
    "    'buttermilk pancakes', 'mango lassi with yogurt', 'cream-based sauces',\n",
    "    'buttery croissants', 'garlic butter shrimp', 'lobster with butter',\n",
    "    'baked brie with honey and nuts'],\n",
    "\n",
    "\n",
    "    'Omnivore': ['beef', 'chicken', 'fish', 'beef', 'chicken', 'pork', 'lamb', 'veal', 'duck', 'turkey', 'rabbit',\n",
    "    'bacon', 'sausage', 'ham', 'salami', 'pepperoni', 'prosciutto',\n",
    "    'fish', 'shrimp', 'lobster', 'crab', 'clams', 'mussels', 'oysters',\n",
    "    'anchovies', 'sardines', 'herring', 'tuna', 'caviar', 'escargot',\n",
    "    'bone broth', 'lard', 'schmaltz', 'animal fat', 'ghee']\n",
    "\n",
    "    # Add more diet types and their associated keywords as needed\n",
    "}\n",
    "\n",
    "# Function to determine diet type based on keywords\n",
    "def infer_diet(ingredients):\n",
    "    omnivore_keywords = diet_keywords.get('Omnivore', [])\n",
    "    vegetarian_keywords = diet_keywords.get('Vegetarian', [])\n",
    "\n",
    "    # Check for omnivore keywords first\n",
    "    if any(keyword in ingredients.lower() for keyword in omnivore_keywords):\n",
    "        return 'Omnivore'\n",
    "    # Check for vegetarian keywords if not omnivore\n",
    "    elif any(keyword in ingredients.lower() for keyword in vegetarian_keywords):\n",
    "        return 'Vegetarian'\n",
    "    else:\n",
    "        return 'Vegan'  # Default to Vegan if no diet type is inferred\n",
    "    \n",
    "# Apply the function to create the 'Diet' column\n",
    "cleaned_df_ingredient['meat_vegan_veggie'] = cleaned_df_ingredient['CleanedIngredients'].apply(infer_diet)\n",
    "cleaned_df_ingredient['RecipeId'] = df_recipes['RecipeId']\n",
    "cleaned_df_ingredient['Omnivore'] = 0\n",
    "cleaned_df_ingredient['Vegeterian'] = 0\n",
    "cleaned_df_ingredient['Vegan'] = 0\n",
    "cleaned_df_ingredient.loc[cleaned_df_ingredient['meat_vegan_veggie'] == 'Omnivore', 'Omnivore'] = 1\n",
    "cleaned_df_ingredient.loc[cleaned_df_ingredient['meat_vegan_veggie'] == 'Vegan', ['Omnivore','Vegeterian','Vegan']] = 1\n",
    "cleaned_df_ingredient.loc[cleaned_df_ingredient['meat_vegan_veggie'] == 'Vegeterian', ['Vegeterian','Omnivore']] = 1 \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.4. Automatic column diet mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" df_temp = df_result_i.copy()\\ndf_temp = df_temp.drop('CleanedIngredients',axis=1)\\nunique_ingredients_list = pd.unique(df_temp.values.flatten()).tolist() \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" df_temp = df_result_i.copy()\n",
    "df_temp = df_temp.drop('CleanedIngredients',axis=1)\n",
    "unique_ingredients_list = pd.unique(df_temp.values.flatten()).tolist() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' cleaned_df_ingredient = df_result_i.copy()\\n# Fill NaN values with an empty string\\ncleaned_df_ingredient[\\'CleanedIngredients\\'] = cleaned_df_ingredient[\\'CleanedIngredients\\'].fillna(\\'\\')\\n\\n# Loop through the ingredient names and create columns\\nfor ingredient in unique_ingredients_list:\\n    try:\\n        cleaned_df_ingredient[ingredient] = cleaned_df_ingredient[\\'CleanedIngredients\\'].apply(lambda x: 1 if x is not None and ingredient in str(x) else 0)\\n    except Exception as e:\\n        print(f\"Error processing ingredient \\'{ingredient}\\': {e}\")\\n\\ncleaned_df_ingredient = cleaned_df_ingredient.filter(regex=\\'^(?!Ingredient)\\')\\ncleaned_df_ingredient[\\'RecipeId\\'] = df_recipes[\\'RecipeId\\']\\ncleaned_df_ingredient = cleaned_df_ingredient.drop(\\'CleanedIngredients\\', axis=1) '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" cleaned_df_ingredient = df_result_i.copy()\n",
    "# Fill NaN values with an empty string\n",
    "cleaned_df_ingredient['CleanedIngredients'] = cleaned_df_ingredient['CleanedIngredients'].fillna('')\n",
    "\n",
    "# Loop through the ingredient names and create columns\n",
    "for ingredient in unique_ingredients_list:\n",
    "    try:\n",
    "        cleaned_df_ingredient[ingredient] = cleaned_df_ingredient['CleanedIngredients'].apply(lambda x: 1 if x is not None and ingredient in str(x) else 0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ingredient '{ingredient}': {e}\")\n",
    "\n",
    "cleaned_df_ingredient = cleaned_df_ingredient.filter(regex='^(?!Ingredient)')\n",
    "cleaned_df_ingredient['RecipeId'] = df_recipes['RecipeId']\n",
    "cleaned_df_ingredient = cleaned_df_ingredient.drop('CleanedIngredients', axis=1) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.5. Improved mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_classification_csv(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df.set_index('Ingredient').to_dict()['Diet Classification']\n",
    "\n",
    "def classify_dietary_category(ingredients_list, classification_dict):\n",
    "    ingredients = [ingredient.strip() for ingredient in ingredients_list.split(\",\")]\n",
    "    if any(classification_dict.get(ingredient, 'Vegan') == 'Omnivore' for ingredient in ingredients):\n",
    "        return 'Omnivore'\n",
    "    elif any(classification_dict.get(ingredient, 'Vegan') == 'Vegetarian' for ingredient in ingredients):\n",
    "        return 'Vegetarian'\n",
    "    else:\n",
    "        return 'Vegan'\n",
    "\n",
    "# Read the classification CSV file\n",
    "csv_file_path = \"data/classified_ingredients.csv\"  # Adjust the path as needed\n",
    "classification_dict = read_classification_csv(csv_file_path)\n",
    "\n",
    "# Apply the classification function to the 'CleanedIngredients' column of df_result_i\n",
    "df_result_i['Diet Classification'] = df_result_i['CleanedIngredients'].apply(lambda x: classify_dietary_category(x, classification_dict))\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "cleaned_df_ingredient = df_result_i[['CleanedIngredients','Diet Classification']].copy()\n",
    "cleaned_df_ingredient = cleaned_df_ingredient.filter(regex='^(?!Ingredient)')\n",
    "cleaned_df_ingredient['RecipeId'] = df_recipes['RecipeId']\n",
    "cleaned_df_ingredient = cleaned_df_ingredient.drop('CleanedIngredients', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.6. Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to modify imports\n",
    "recipes = df_recipes.copy()\n",
    "reviews = df_reviews.copy()\n",
    "diet = df_diet.copy()\n",
    "requests = df_requests.copy()\n",
    "\n",
    "df_unprocessed = pd.merge(reviews, requests, on=['AuthorId','RecipeId'], how='inner')\n",
    "df_unprocessed = pd.merge(df_unprocessed, recipes, on='RecipeId', how='left')\n",
    "df_unprocessed = pd.merge(df_unprocessed, cleaned_df_ingredient, on='RecipeId', how='left')\n",
    "df_unprocessed = pd.merge(df_unprocessed, diet, on='AuthorId', how='left')\n",
    "\n",
    "merged_data = df_unprocessed.copy()\n",
    "# Test DFs\n",
    "prediction_on_revies = reviews.dropna(subset=[\"TestSetId\"])\n",
    "#reviews_got = reviews.dropna(subset=[\"Like\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.7. Modify columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\baml-venv\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Add new column is_cooking_time_within_expectation\n",
    "merged_data['is_cooking_time_within_expectation'] = (merged_data['CookTime'] + merged_data['PrepTime']) > merged_data['Time']\n",
    "# Data preprocessing\n",
    "# Drop irrelevant columns and handle missing values\n",
    "merged_data = merged_data.drop(['Name',\n",
    "                                'RecipeYield',\n",
    "                                'CookTime',\n",
    "                                'PrepTime',\n",
    "                                'Time',\n",
    "                                'RecipeServings',\n",
    "                               # 'Age',\n",
    "                               # 'CleanedIngredients',\n",
    "                                'TestSetId',\n",
    "                                'AuthorId',\n",
    "                                'RecipeId',\n",
    "                                'RecipeIngredientQuantities',\n",
    "                                'RecipeIngredientParts',\n",
    "                                'Rating'], \n",
    "                                axis=1)\n",
    "\n",
    "# Assuming merged_data is your DataFrame\n",
    "data_to_encode = merged_data[['Diet', \n",
    "                              'RecipeCategory', \n",
    "                              'LowSugar', \n",
    "                              'HighProtein',\n",
    "                              'Diet Classification'\n",
    "                              ]].copy()\n",
    "\n",
    "# Create OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "data_encoded = onehot_encoder.fit_transform(data_to_encode)\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded columns\n",
    "data_encoded_df = pd.DataFrame(data_encoded, \n",
    "                               columns=onehot_encoder.get_feature_names_out(data_to_encode.columns))\n",
    "\n",
    "# Drop the original columns from the DataFrame\n",
    "merged_data = merged_data.drop(columns=['Diet', \n",
    "                                        'RecipeCategory', \n",
    "                                        'LowSugar', \n",
    "                                        'HighProtein',\n",
    "                                        'Diet Classification'\n",
    "                                        ])\n",
    "\n",
    "# Concatenate the one-hot encoded columns with the original DataFrame\n",
    "merged_data = pd.concat([merged_data, \n",
    "                         data_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.9 Check columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Like', 'HighCalories', 'LowFat', 'HighFiber', 'Calories', 'FatContent',\n",
       "       'SaturatedFatContent', 'CholesterolContent', 'SodiumContent',\n",
       "       'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent',\n",
       "       'Age', 'is_cooking_time_within_expectation', 'Diet_Omnivore',\n",
       "       'Diet_Vegan', 'Diet_Vegetarian', 'RecipeCategory_Beverages',\n",
       "       'RecipeCategory_Bread', 'RecipeCategory_Breakfast',\n",
       "       'RecipeCategory_Lunch', 'RecipeCategory_One dish meal',\n",
       "       'RecipeCategory_Other', 'RecipeCategory_Soup', 'LowSugar_0',\n",
       "       'LowSugar_Indifferent', 'HighProtein_Indifferent', 'HighProtein_Yes',\n",
       "       'Diet Classification_Omnivore', 'Diet Classification_Vegan',\n",
       "       'Diet Classification_Vegetarian'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column check on \n",
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1. Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_20580\\4161558566.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_training['Like'] = label_encoder.fit_transform(X_training['Like'])\n"
     ]
    }
   ],
   "source": [
    "# Define features and target variable\n",
    "X_predict = merged_data.copy()\n",
    "X_training = merged_data.dropna()\n",
    "label_encoder = LabelEncoder()\n",
    "X_training['Like'] = label_encoder.fit_transform(X_training['Like'])\n",
    "X = X_training.drop('Like', axis=1)\n",
    "y = X_training['Like']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "Balanced Accuracy: 0.67598970\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.9077312218189816, 'recall': 0.9818592755605782, 'f1-score': 0.9433412383210948, 'support': 16813.0}, '1': {'precision': 0.7637490317583269, 'recall': 0.37012012012012013, 'f1-score': 0.4986093552465234, 'support': 2664.0}, 'accuracy': 0.8981876058941315, 'macro avg': {'precision': 0.8357401267886542, 'recall': 0.6759896978403491, 'f1-score': 0.7209752967838091, 'support': 19477.0}, 'weighted avg': {'precision': 0.888037811420995, 'recall': 0.8981876058941315, 'f1-score': 0.8825122740806749, 'support': 19477.0}}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "XGBoost:\n",
      "Balanced Accuracy: 0.74290904\n",
      "Classification Report:\n",
      "{'0': {'precision': 0.9269140869961804, 'recall': 0.9670493070838042, 'f1-score': 0.9465564417535076, 'support': 16813.0}, '1': {'precision': 0.7138429752066116, 'recall': 0.5187687687687688, 'f1-score': 0.6008695652173913, 'support': 2664.0}, 'accuracy': 0.9057349694511475, 'macro avg': {'precision': 0.820378531101396, 'recall': 0.7429090379262865, 'f1-score': 0.7737130034854495, 'support': 19477.0}, 'weighted avg': {'precision': 0.897770921118098, 'recall': 0.9057349694511475, 'f1-score': 0.8992745277476436, 'support': 19477.0}}\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': XGBClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "\n",
    "best_model = None\n",
    "best_balanced_acc = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    results[name] = {\n",
    "        'Balanced Accuracy': balanced_acc,\n",
    "        'Classification Report': report\n",
    "    }\n",
    "\n",
    "    # Save the best model based on balanced accuracy\n",
    "    if balanced_acc > best_balanced_acc:\n",
    "        best_model = model\n",
    "        best_balanced_acc = balanced_acc\n",
    "\n",
    "# Display results\n",
    "for name, result in results.items():\n",
    "    print(f'{name}:')\n",
    "    print(f'Balanced Accuracy: {result[\"Balanced Accuracy\"]:.8f}')\n",
    "    print('Classification Report:')\n",
    "    print(result['Classification Report'])\n",
    "    print('\\n' + '-'*50 + '\\n')\n",
    "\n",
    "private_test_predictions = pd.DataFrame({'True_Label': y_test, 'Predicted_Label': y_pred})\n",
    "private_test_predictions.to_csv('data/Results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediciton\n",
    "# Make predictions on the test set\n",
    "X_predict = X_predict.drop('Like', axis=1)\n",
    "y_pred = model.predict(X_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = pd.DataFrame({'id': prediction_on_revies[\"TestSetId\"].astype(int), 'prediction': y_pred})\n",
    "print(test_predictions)\n",
    "test_predictions.to_csv('data/Results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
